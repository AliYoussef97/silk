<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>silk.transforms.tensor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>silk.transforms.tensor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from typing import Any, Iterable, List, Optional, Tuple, Union

import numpy as np
import torch
from PIL.Image import Image as PILImage
from silk.transforms.abstract import Transform, NamedContext


class ToTensor(Transform):
    &#34;&#34;&#34;Transform item to pytorch tensor.&#34;&#34;&#34;

    def __init__(
        self,
        dtype: Union[str, None] = None,
        device: Union[str, None] = None,
        requires_grad: bool = False,
    ) -&gt; None:
        super().__init__()
        self._dtype = dtype
        self._device = device
        self._requires_grad = requires_grad

    def __call__(self, item: Any) -&gt; Any:
        # if PIL image, convert to numpy array
        if isinstance(item, PILImage):
            item = np.array(item)
        elif isinstance(item, torch.Tensor):
            item = item.to(self._device)
            item = item.to(self._dtype)
            item.requires_grad_(self._requires_grad)

        if isinstance(item, torch.Tensor):
            return item
        return torch.tensor(
            item,
            dtype=self._dtype,
            device=self._device,
            requires_grad=self._requires_grad,
        )


class ToDevice(Transform):
    &#34;&#34;&#34;Sends tensor(s) to specified device. Handle batch dimension as tuple or list.&#34;&#34;&#34;

    def __init__(self, device: str) -&gt; None:
        super().__init__()
        self.device = device

    def __call__(
        self, item: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]
    ) -&gt; Any:
        if isinstance(item, list):
            return [it.to(self.device) for it in item]
        if isinstance(item, tuple):
            return tuple(it.to(self.device) for it in item)
        return item.to(self.device)


class Shape(Transform):
    def __init__(self, start=None, end=None) -&gt; None:
        super().__init__()
        self._start = start
        self._end = end

    def __call__(self, item: torch.Tensor) -&gt; Any:
        start = 0 if self._start is None else self._start
        end = len(item.shape) if self._end is None else self._end
        return torch.tensor(item.shape[start:end])


class AutoBatch(Transform):
    &#34;&#34;&#34;Transform useful to batch data coming from a data loader.

    * Input are either single item or list of items.
    * Single item are processed as being a list of one item.
    * Numerical items (int, float) are batched as a torch tensor.
    * Torch tensor items are batch as a new torch tensor having a new batch dimension (dimension 0).
    * Strings items are batched as a tuple of strings.
    * Dictionary items are recursively batched accross all of their keys. All dictionary items should have same keys.
    * Tuple items are recursively batched accross all of their positions. All tuple items should have same size.
    * NamedContext items are recursively batched accross all of their named variables. All named context item should have same variable names.
    * Lists are not handled since they represent the batch dimension.

    Examples
    --------

    ```python
    from silk.transforms.abstract import AutoBatch, NamedContext

    transf = AutoBatch()

    print(transf([1, 2, 3]))
    # &gt;&gt;&gt; tensor([1, 2, 3])

    print(transf(1))
    # &gt;&gt;&gt; tensor([1])

    batch = [ NamedContext(a=float(i), b=str(i)) for i in range(4) ]
    print(batch)
    # &gt;&gt;&gt; [NamedContext({&#39;a&#39;: 0.0, &#39;b&#39;: &#39;0&#39;}), NamedContext({&#39;a&#39;: 1.0, &#39;b&#39;: &#39;1&#39;}), NamedContext({&#39;a&#39;: 2.0, &#39;b&#39;: &#39;2&#39;}), NamedContext({&#39;a&#39;: 3.0, &#39;b&#39;: &#39;3&#39;})]
    print(transf(batch))
    # &gt;&gt;&gt; NamedContext({&#39;a&#39;: tensor([0., 1., 2., 3.], dtype=torch.float64), &#39;b&#39;: (&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;)})
    ```
    &#34;&#34;&#34;

    def __init__(self, transform: Optional[Transform] = None) -&gt; None:
        &#34;&#34;&#34;

        Parameters
        ----------
        transform : Optional[Transform], optional
            Optional transform to apply to each item in batch, by default None
        &#34;&#34;&#34;
        super().__init__()

        def do_nothing(x):
            return x

        self._transform = do_nothing if transform is None else transform

    def __call__(self, item: Any) -&gt; Any:
        &#34;&#34;&#34;Run the optional transform and auto-batch the input item.

        Parameters
        ----------
        item : Any
            Input to auto-batch.

        Returns
        -------
        Any
            Batched output.

        Raises
        ------
        RuntimeError
            When provided item is an empty list.
        &#34;&#34;&#34;
        if isinstance(item, list) or isinstance(item, tuple):
            if not len(item) &gt; 0:
                raise RuntimeError(&#34;provided item is empty&#34;)

            transformed = [self._transform(it) for it in item]
        else:
            transformed = [self._transform(item)]

        return AutoBatch._collate(transformed)

    @staticmethod
    def _collate(batch):
        r&#34;&#34;&#34;Puts each data field into a tensor with outer dimension batch size&#34;&#34;&#34;

        elem = batch[0]
        elem_type = type(elem)
        if isinstance(elem, torch.Tensor):
            if all(el.shape == elem.shape for el in batch):
                # returns tensor with first dimension as batch dimension
                return torch.stack(batch, 0)
            # returns batch of tensors having different shapes
            return list(batch)
        elif isinstance(elem, float):
            return torch.tensor(batch, dtype=torch.float64)
        elif isinstance(elem, int):
            return torch.tensor(batch)
        elif isinstance(elem, str):
            return list(batch)
        elif isinstance(elem, dict):
            return {key: AutoBatch._collate([d[key] for d in batch]) for key in elem}
        elif isinstance(elem, tuple):
            # check to make sure that the elements in batch have consistent size
            elem_size = len(elem)
            if not all(len(el) == elem_size for el in batch):
                raise RuntimeError(
                    &#34;each element in list of batch should be of equal size&#34;
                )
            return tuple(AutoBatch._collate(samples) for samples in zip(*batch))
        elif isinstance(elem, NamedContext):
            ctx = NamedContext.batching(batch)
            return ctx.map(AutoBatch._collate)
        elif isinstance(elem, Iterable):
            return list(batch)
        elif isinstance(elem, type(None)):
            return list(batch)

        raise TypeError(
            f&#34;batch must contain tensors, numbers, dicts or lists; found {elem_type}&#34;
        )


class Unbatch(Transform):
    &#34;&#34;&#34;Reverse operator of `AutoBatch`.&#34;&#34;&#34;

    def __init__(self, tuple_as_list: bool = False) -&gt; None:
        super().__init__()
        self._tuple_as_list = tuple_as_list

    def __call__(self, item: Any) -&gt; Any:
        return Unbatch._uncollate(item, self._tuple_as_list)

    @staticmethod
    def _uncollate(batched_item, tuple_as_list):
        if torch.is_tensor(batched_item):
            return [batched_item[i] for i in range(batched_item.shape[0])]
        elif isinstance(batched_item, list):
            return batched_item
        elif isinstance(batched_item, tuple):
            if tuple_as_list:
                return batched_item
            items = tuple(
                Unbatch._uncollate(batched_item[i], tuple_as_list)
                for i in range(len(batched_item))
            )

            if not all(len(items[i]) == len(items[0]) for i in range(len(items))):
                raise RuntimeError(
                    &#34;each element in list of batch should be of equal size&#34;
                )

            return [
                tuple(items[i][j] for i in range(len(items)))
                for j in range(len(items[0]))
            ]

        elif isinstance(batched_item, dict):
            items = {
                key: Unbatch._uncollate(batched_item[key], tuple_as_list)
                for key in batched_item
            }
            batch_size = len(items[next(iter(items.keys()))])
            return [
                {key: items[key][i] for key in batched_item} for i in range(batch_size)
            ]
        elif isinstance(batched_item, NamedContext):
            items = {
                key: Unbatch._uncollate(batched_item[key], tuple_as_list)
                for key in batched_item.names()
            }
            batch_size = len(items[next(iter(items.keys()))])
            return [
                NamedContext({key: items[key][i] for key in items})
                for i in range(batch_size)
            ]
        elif (
            isinstance(batched_item, float)
            or isinstance(batched_item, int)
            or isinstance(batched_item, str)
        ):
            return batched_item

        raise TypeError(
            f&#34;batched item must either be a tensor, number, dict or list; found {type(batched_item)}&#34;
        )


class NormalizeRange(Transform):
    &#34;&#34;&#34;Normalize input range to new range.&#34;&#34;&#34;

    def __init__(self, ilow, ihigh, olow=0.0, ohigh=1.0) -&gt; None:
        super().__init__()

        self._ilow = ilow
        self._ihigh = ihigh
        self._olow = olow
        self._ohigh = ohigh

        self._alpha = (ohigh - olow) / (ihigh - ilow)
        self._beta = olow - ilow * self._alpha

    def __call__(self, item: Any) -&gt; Any:
        return item * self._alpha + self._beta


class Clamp(Transform):
    &#34;&#34;&#34;Clamp values to make sure they get out of range.&#34;&#34;&#34;

    def __init__(
        self,
        min_val: Union[float, None] = None,
        max_val: Union[float, None] = None,
    ) -&gt; None:
        super().__init__()

        self._min_val = min_val
        self._max_val = max_val

    def __call__(self, item: torch.Tensor) -&gt; Any:
        if (self._max_val is not None) or (self._min_val is not None):
            item = item.clamp_(self._min_val, self._max_val)
        return item</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="silk.transforms.tensor.AutoBatch"><code class="flex name class">
<span>class <span class="ident">AutoBatch</span></span>
<span>(</span><span>transform: Optional[<a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform useful to batch data coming from a data loader.</p>
<ul>
<li>Input are either single item or list of items.</li>
<li>Single item are processed as being a list of one item.</li>
<li>Numerical items (int, float) are batched as a torch tensor.</li>
<li>Torch tensor items are batch as a new torch tensor having a new batch dimension (dimension 0).</li>
<li>Strings items are batched as a tuple of strings.</li>
<li>Dictionary items are recursively batched accross all of their keys. All dictionary items should have same keys.</li>
<li>Tuple items are recursively batched accross all of their positions. All tuple items should have same size.</li>
<li>NamedContext items are recursively batched accross all of their named variables. All named context item should have same variable names.</li>
<li>Lists are not handled since they represent the batch dimension.</li>
</ul>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">from silk.transforms.abstract import AutoBatch, NamedContext

transf = AutoBatch()

print(transf([1, 2, 3]))
# &gt;&gt;&gt; tensor([1, 2, 3])

print(transf(1))
# &gt;&gt;&gt; tensor([1])

batch = [ NamedContext(a=float(i), b=str(i)) for i in range(4) ]
print(batch)
# &gt;&gt;&gt; [NamedContext({'a': 0.0, 'b': '0'}), NamedContext({'a': 1.0, 'b': '1'}), NamedContext({'a': 2.0, 'b': '2'}), NamedContext({'a': 3.0, 'b': '3'})]
print(transf(batch))
# &gt;&gt;&gt; NamedContext({'a': tensor([0., 1., 2., 3.], dtype=torch.float64), 'b': ('0', '1', '2', '3')})
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>transform</code></strong> :&ensp;<code>Optional[Transform]</code>, optional</dt>
<dd>Optional transform to apply to each item in batch, by default None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoBatch(Transform):
    &#34;&#34;&#34;Transform useful to batch data coming from a data loader.

    * Input are either single item or list of items.
    * Single item are processed as being a list of one item.
    * Numerical items (int, float) are batched as a torch tensor.
    * Torch tensor items are batch as a new torch tensor having a new batch dimension (dimension 0).
    * Strings items are batched as a tuple of strings.
    * Dictionary items are recursively batched accross all of their keys. All dictionary items should have same keys.
    * Tuple items are recursively batched accross all of their positions. All tuple items should have same size.
    * NamedContext items are recursively batched accross all of their named variables. All named context item should have same variable names.
    * Lists are not handled since they represent the batch dimension.

    Examples
    --------

    ```python
    from silk.transforms.abstract import AutoBatch, NamedContext

    transf = AutoBatch()

    print(transf([1, 2, 3]))
    # &gt;&gt;&gt; tensor([1, 2, 3])

    print(transf(1))
    # &gt;&gt;&gt; tensor([1])

    batch = [ NamedContext(a=float(i), b=str(i)) for i in range(4) ]
    print(batch)
    # &gt;&gt;&gt; [NamedContext({&#39;a&#39;: 0.0, &#39;b&#39;: &#39;0&#39;}), NamedContext({&#39;a&#39;: 1.0, &#39;b&#39;: &#39;1&#39;}), NamedContext({&#39;a&#39;: 2.0, &#39;b&#39;: &#39;2&#39;}), NamedContext({&#39;a&#39;: 3.0, &#39;b&#39;: &#39;3&#39;})]
    print(transf(batch))
    # &gt;&gt;&gt; NamedContext({&#39;a&#39;: tensor([0., 1., 2., 3.], dtype=torch.float64), &#39;b&#39;: (&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;)})
    ```
    &#34;&#34;&#34;

    def __init__(self, transform: Optional[Transform] = None) -&gt; None:
        &#34;&#34;&#34;

        Parameters
        ----------
        transform : Optional[Transform], optional
            Optional transform to apply to each item in batch, by default None
        &#34;&#34;&#34;
        super().__init__()

        def do_nothing(x):
            return x

        self._transform = do_nothing if transform is None else transform

    def __call__(self, item: Any) -&gt; Any:
        &#34;&#34;&#34;Run the optional transform and auto-batch the input item.

        Parameters
        ----------
        item : Any
            Input to auto-batch.

        Returns
        -------
        Any
            Batched output.

        Raises
        ------
        RuntimeError
            When provided item is an empty list.
        &#34;&#34;&#34;
        if isinstance(item, list) or isinstance(item, tuple):
            if not len(item) &gt; 0:
                raise RuntimeError(&#34;provided item is empty&#34;)

            transformed = [self._transform(it) for it in item]
        else:
            transformed = [self._transform(item)]

        return AutoBatch._collate(transformed)

    @staticmethod
    def _collate(batch):
        r&#34;&#34;&#34;Puts each data field into a tensor with outer dimension batch size&#34;&#34;&#34;

        elem = batch[0]
        elem_type = type(elem)
        if isinstance(elem, torch.Tensor):
            if all(el.shape == elem.shape for el in batch):
                # returns tensor with first dimension as batch dimension
                return torch.stack(batch, 0)
            # returns batch of tensors having different shapes
            return list(batch)
        elif isinstance(elem, float):
            return torch.tensor(batch, dtype=torch.float64)
        elif isinstance(elem, int):
            return torch.tensor(batch)
        elif isinstance(elem, str):
            return list(batch)
        elif isinstance(elem, dict):
            return {key: AutoBatch._collate([d[key] for d in batch]) for key in elem}
        elif isinstance(elem, tuple):
            # check to make sure that the elements in batch have consistent size
            elem_size = len(elem)
            if not all(len(el) == elem_size for el in batch):
                raise RuntimeError(
                    &#34;each element in list of batch should be of equal size&#34;
                )
            return tuple(AutoBatch._collate(samples) for samples in zip(*batch))
        elif isinstance(elem, NamedContext):
            ctx = NamedContext.batching(batch)
            return ctx.map(AutoBatch._collate)
        elif isinstance(elem, Iterable):
            return list(batch)
        elif isinstance(elem, type(None)):
            return list(batch)

        raise TypeError(
            f&#34;batch must contain tensors, numbers, dicts or lists; found {elem_type}&#34;
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.AutoBatch.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.AutoBatch.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.Clamp"><code class="flex name class">
<span>class <span class="ident">Clamp</span></span>
<span>(</span><span>min_val: Optional[float] = None, max_val: Optional[float] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Clamp values to make sure they get out of range.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Clamp(Transform):
    &#34;&#34;&#34;Clamp values to make sure they get out of range.&#34;&#34;&#34;

    def __init__(
        self,
        min_val: Union[float, None] = None,
        max_val: Union[float, None] = None,
    ) -&gt; None:
        super().__init__()

        self._min_val = min_val
        self._max_val = max_val

    def __call__(self, item: torch.Tensor) -&gt; Any:
        if (self._max_val is not None) or (self._min_val is not None):
            item = item.clamp_(self._min_val, self._max_val)
        return item</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.Clamp.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.Clamp.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.NormalizeRange"><code class="flex name class">
<span>class <span class="ident">NormalizeRange</span></span>
<span>(</span><span>ilow, ihigh, olow=0.0, ohigh=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalize input range to new range.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NormalizeRange(Transform):
    &#34;&#34;&#34;Normalize input range to new range.&#34;&#34;&#34;

    def __init__(self, ilow, ihigh, olow=0.0, ohigh=1.0) -&gt; None:
        super().__init__()

        self._ilow = ilow
        self._ihigh = ihigh
        self._olow = olow
        self._ohigh = ohigh

        self._alpha = (ohigh - olow) / (ihigh - ilow)
        self._beta = olow - ilow * self._alpha

    def __call__(self, item: Any) -&gt; Any:
        return item * self._alpha + self._beta</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.NormalizeRange.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.NormalizeRange.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
<span>(</span><span>start=None, end=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract representation of a transform, which is essentially a parametrized function taking one input.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape(Transform):
    def __init__(self, start=None, end=None) -&gt; None:
        super().__init__()
        self._start = start
        self._end = end

    def __call__(self, item: torch.Tensor) -&gt; Any:
        start = 0 if self._start is None else self._start
        end = len(item.shape) if self._end is None else self._end
        return torch.tensor(item.shape[start:end])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.Shape.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.Shape.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.ToDevice"><code class="flex name class">
<span>class <span class="ident">ToDevice</span></span>
<span>(</span><span>device: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Sends tensor(s) to specified device. Handle batch dimension as tuple or list.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ToDevice(Transform):
    &#34;&#34;&#34;Sends tensor(s) to specified device. Handle batch dimension as tuple or list.&#34;&#34;&#34;

    def __init__(self, device: str) -&gt; None:
        super().__init__()
        self.device = device

    def __call__(
        self, item: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]
    ) -&gt; Any:
        if isinstance(item, list):
            return [it.to(self.device) for it in item]
        if isinstance(item, tuple):
            return tuple(it.to(self.device) for it in item)
        return item.to(self.device)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.ToDevice.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.ToDevice.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.ToTensor"><code class="flex name class">
<span>class <span class="ident">ToTensor</span></span>
<span>(</span><span>dtype: Optional[str] = None, device: Optional[str] = None, requires_grad: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Transform item to pytorch tensor.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ToTensor(Transform):
    &#34;&#34;&#34;Transform item to pytorch tensor.&#34;&#34;&#34;

    def __init__(
        self,
        dtype: Union[str, None] = None,
        device: Union[str, None] = None,
        requires_grad: bool = False,
    ) -&gt; None:
        super().__init__()
        self._dtype = dtype
        self._device = device
        self._requires_grad = requires_grad

    def __call__(self, item: Any) -&gt; Any:
        # if PIL image, convert to numpy array
        if isinstance(item, PILImage):
            item = np.array(item)
        elif isinstance(item, torch.Tensor):
            item = item.to(self._device)
            item = item.to(self._dtype)
            item.requires_grad_(self._requires_grad)

        if isinstance(item, torch.Tensor):
            return item
        return torch.tensor(
            item,
            dtype=self._dtype,
            device=self._device,
            requires_grad=self._requires_grad,
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.ToTensor.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.ToTensor.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="silk.transforms.tensor.Unbatch"><code class="flex name class">
<span>class <span class="ident">Unbatch</span></span>
<span>(</span><span>tuple_as_list: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Reverse operator of <code><a title="silk.transforms.tensor.AutoBatch" href="#silk.transforms.tensor.AutoBatch">AutoBatch</a></code>.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Unbatch(Transform):
    &#34;&#34;&#34;Reverse operator of `AutoBatch`.&#34;&#34;&#34;

    def __init__(self, tuple_as_list: bool = False) -&gt; None:
        super().__init__()
        self._tuple_as_list = tuple_as_list

    def __call__(self, item: Any) -&gt; Any:
        return Unbatch._uncollate(item, self._tuple_as_list)

    @staticmethod
    def _uncollate(batched_item, tuple_as_list):
        if torch.is_tensor(batched_item):
            return [batched_item[i] for i in range(batched_item.shape[0])]
        elif isinstance(batched_item, list):
            return batched_item
        elif isinstance(batched_item, tuple):
            if tuple_as_list:
                return batched_item
            items = tuple(
                Unbatch._uncollate(batched_item[i], tuple_as_list)
                for i in range(len(batched_item))
            )

            if not all(len(items[i]) == len(items[0]) for i in range(len(items))):
                raise RuntimeError(
                    &#34;each element in list of batch should be of equal size&#34;
                )

            return [
                tuple(items[i][j] for i in range(len(items)))
                for j in range(len(items[0]))
            ]

        elif isinstance(batched_item, dict):
            items = {
                key: Unbatch._uncollate(batched_item[key], tuple_as_list)
                for key in batched_item
            }
            batch_size = len(items[next(iter(items.keys()))])
            return [
                {key: items[key][i] for key in batched_item} for i in range(batch_size)
            ]
        elif isinstance(batched_item, NamedContext):
            items = {
                key: Unbatch._uncollate(batched_item[key], tuple_as_list)
                for key in batched_item.names()
            }
            batch_size = len(items[next(iter(items.keys()))])
            return [
                NamedContext({key: items[key][i] for key in items})
                for i in range(batch_size)
            ]
        elif (
            isinstance(batched_item, float)
            or isinstance(batched_item, int)
            or isinstance(batched_item, str)
        ):
            return batched_item

        raise TypeError(
            f&#34;batched item must either be a tensor, number, dict or list; found {type(batched_item)}&#34;
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="silk.transforms.tensor.Unbatch.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="silk.transforms.tensor.Unbatch.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="silk.transforms.abstract.Transform" href="abstract.html#silk.transforms.abstract.Transform">Transform</a></b></code>:
<ul class="hlist">
<li><code><a title="silk.transforms.abstract.Transform.forward" href="abstract.html#silk.transforms.abstract.Transform.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="silk.transforms" href="index.html">silk.transforms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="silk.transforms.tensor.AutoBatch" href="#silk.transforms.tensor.AutoBatch">AutoBatch</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.AutoBatch.dump_patches" href="#silk.transforms.tensor.AutoBatch.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.AutoBatch.training" href="#silk.transforms.tensor.AutoBatch.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.Clamp" href="#silk.transforms.tensor.Clamp">Clamp</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.Clamp.dump_patches" href="#silk.transforms.tensor.Clamp.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.Clamp.training" href="#silk.transforms.tensor.Clamp.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.NormalizeRange" href="#silk.transforms.tensor.NormalizeRange">NormalizeRange</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.NormalizeRange.dump_patches" href="#silk.transforms.tensor.NormalizeRange.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.NormalizeRange.training" href="#silk.transforms.tensor.NormalizeRange.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.Shape" href="#silk.transforms.tensor.Shape">Shape</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.Shape.dump_patches" href="#silk.transforms.tensor.Shape.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.Shape.training" href="#silk.transforms.tensor.Shape.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.ToDevice" href="#silk.transforms.tensor.ToDevice">ToDevice</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.ToDevice.dump_patches" href="#silk.transforms.tensor.ToDevice.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.ToDevice.training" href="#silk.transforms.tensor.ToDevice.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.ToTensor" href="#silk.transforms.tensor.ToTensor">ToTensor</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.ToTensor.dump_patches" href="#silk.transforms.tensor.ToTensor.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.ToTensor.training" href="#silk.transforms.tensor.ToTensor.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="silk.transforms.tensor.Unbatch" href="#silk.transforms.tensor.Unbatch">Unbatch</a></code></h4>
<ul class="">
<li><code><a title="silk.transforms.tensor.Unbatch.dump_patches" href="#silk.transforms.tensor.Unbatch.dump_patches">dump_patches</a></code></li>
<li><code><a title="silk.transforms.tensor.Unbatch.training" href="#silk.transforms.tensor.Unbatch.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>